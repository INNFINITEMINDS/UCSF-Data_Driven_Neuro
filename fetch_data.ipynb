{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import mne\n",
    "from zipfile import ZipFile\n",
    "import os.path as op\n",
    "import os\n",
    "import nbformat as nb\n",
    "from glob import glob\n",
    "from nbconvert.preprocessors import ExecutePreprocessor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Images for deep learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data already exists...\n"
     ]
    }
   ],
   "source": [
    "# Define data paths\n",
    "data_path = 'https://dl.dropboxusercontent.com/s/iac73fev99y9awt/data.zip?dl=1'\n",
    "path_download = './deep_learning/tmp.zip'\n",
    "path_save = op.expanduser('~/data_ucsf/deep_learning/cells/')\n",
    "\n",
    "if op.exists(path_save):\n",
    "    print('Data already exists...')\n",
    "else:\n",
    "    print('Downloading data')\n",
    "    data = mne.utils._fetch_file(data_path, path_download,)\n",
    "\n",
    "    print('Extracting files to data folder...')\n",
    "    with ZipFile(path_download, 'r') as ff:\n",
    "        ff.extractall(path_save)\n",
    "\n",
    "    # Removing data\n",
    "    os.remove('./deep_learning/tmp.zip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nilearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nilearn import datasets\n",
    "path_data = '/Users/choldgraf/data_ucsf/machine_learning'\n",
    "\n",
    "_ = datasets.fetch_localizer_button_task(data_dir=path_data)\n",
    "_ = datasets.fetch_atlas_smith_2009(data_dir=path_data)\n",
    "_ = datasets.fetch_haxby(data_dir=path_data)\n",
    "_ = datasets.fetch_miyawaki2008(data_dir=path_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset is already in place. If you want to fetch it again please first remove the folder /Users/choldgraf/.dipy/stanford_hardi \n",
      "Dataset is already in place. If you want to fetch it again please first remove the folder /Users/choldgraf/.dipy/stanford_hardi \n",
      "Data size is approximately 12MB\n",
      "Dataset is already in place. If you want to fetch it again please first remove the folder /Users/choldgraf/.dipy/syn_test \n"
     ]
    }
   ],
   "source": [
    "import dipy as dp\n",
    "import dipy.data as dpd\n",
    "_ = dpd.fetch_stanford_t1()\n",
    "_ = dpd.fetch_stanford_hardi()\n",
    "_ = dpd.fetch_syn_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# List the code that fetches stuff for reference\n",
    "This is just to figure out which commands to add above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      " ./image_processing 001-introduction.ipynb \n",
      "\n",
      "remote, local = dpd.fetch_stanford_t1() \n",
      " --- \n",
      "\n",
      "---\n",
      " ./image_processing 002-segmentation.ipynb \n",
      "\n",
      "remote, local = dpd.fetch_stanford_t1() \n",
      " --- \n",
      "\n",
      "---\n",
      " ./image_processing 003-detrending.ipynb \n",
      "\n",
      "remote, local = dpd.fetch_stanford_t1() \n",
      " --- \n",
      "\n",
      "---\n",
      " ./image_processing 004-registration.ipynb \n",
      "\n",
      "from dipy.data import fetch_stanford_hardi, read_stanford_hardi\n",
      "fetch_stanford_hardi()\n",
      "from dipy.data.fetcher import fetch_syn_data, read_syn_data\n",
      "fetch_syn_data() \n",
      " --- \n",
      "\n",
      "---\n",
      " ./machine_learning 2_plot_3d_and_4d_niimg.ipynb \n",
      "\n",
      "data = datasets.fetch_localizer_button_task(data_dir=path_data) \n",
      " --- \n",
      "\n",
      "rsn = datasets.fetch_atlas_smith_2009(data_dir=path_data)['rsn10'] \n",
      " --- \n",
      "\n",
      "---\n",
      " ./machine_learning 3_decoding_haxby.ipynb \n",
      "\n",
      "The `nilearn.datasets.fetch_haxby` function will download the \n",
      " --- \n",
      "\n",
      "# By default the 2nd subject will be fetched\n",
      "haxby_dataset = datasets.fetch_haxby(data_dir=path_data) \n",
      " --- \n",
      "\n",
      "---\n",
      " ./machine_learning 4_encoding_miyawaki.ipynb \n",
      "\n",
      "from nilearn.datasets import fetch_miyawaki2008 \n",
      " --- \n",
      "\n",
      "dataset = fetch_miyawaki2008(data_dir=path_data) \n",
      " --- \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for thisdir, folders, files in os.walk('./'):\n",
    "    if any(ii in thisdir for ii in ('ipynb_checkpoints', '.git')):\n",
    "        continue\n",
    "    for ifile in files:\n",
    "        if not ifile.endswith('ipynb') or 'fetch_data' in ifile:\n",
    "            continue\n",
    "        ntbk = nb.read(op.join(thisdir, ifile), nb.NO_CONVERT)\n",
    "        dir_printed = 0\n",
    "        for cell in ntbk['cells']:\n",
    "            if 'fetch' in cell['source']:\n",
    "                if dir_printed == 0:\n",
    "                    print('---\\n', thisdir, ifile, '\\n')\n",
    "                    dir_printed = 1\n",
    "                lns = [ln for ln in cell['source'].split('\\n') if 'fetch' in ln]\n",
    "                print('\\n'.join(lns), '\\n --- \\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
