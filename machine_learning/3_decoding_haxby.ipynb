{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> credits: originally by Gael Varoquaux\n",
    "> \n",
    "> adapted by: Chris Holdgraf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import pylab as plt\n",
    "import os.path as op\n",
    "path_data = op.expanduser('~/data_ucsf/machine_learning')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# About this tutorial\n",
    "This is a quick introduction to doing neuroimaging machine learning with `nilearn`. It covers the following topics:\n",
    "\n",
    "* Loading a neuroimaging dataset suitable for decoding\n",
    "* Extracting an ROI and vectorizing our 3D data so it may be passed to a `sklearn` object\n",
    "* Using pandas to munge some data\n",
    "* Fitting a `sklearn` classifier on our neuroimaging data\n",
    "* Visualizing classifier weights on the subject's brain\n",
    "\n",
    "Note: A lot of the material in this tutorial is drawn from the `nilearn` collection of examples. Open-source packages can be a great way to learn both about a package, and about the things that package tries to do (e.g., machine learning).\n",
    "\n",
    "In addition, many `nilearn` developers have recently released a paper covering the topics of decoding brain states with fMRI (and other modalities).\n",
    "\n",
    "* [Link to original `nilearn` tutorial](https://nilearn.github.io/auto_examples/plot_decoding_tutorial.html#sphx-glr-auto-examples-plot-decoding-tutorial-py)\n",
    "* [Link to Varoquaux decoding paper](https://arxiv.org/pdf/1606.05201v2.pdf)\n",
    "\n",
    "# A introduction tutorial to fMRI decoding\n",
    "Thus far we've covered the general topics of machine learning, but how do they apply specifically to neuroscience data? There are many packages that have been designed to leverage the power of machine learning for the specific problems we attempt to solve in neuroscience. One excellent package is called `nilearn`, which focuses on neuroimaging.\n",
    "\n",
    "This is a simple tutorial on decoding with nilearn. It reproduces the\n",
    "Haxby 2001 study on a face vs cat discrimination task in a mask of the\n",
    "ventral stream. We'll cover some basics of how to represent neuroimaging data in python, as well as some high-level functions that allow you to decode experimental conditions with this data.\n",
    "\n",
    "* [Here's a link](http://www.pymvpa.org/datadb/haxby2001.html) to the Haxby 2001 Dataset (w/ a link to the paper too)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing the data\n",
    "\n",
    "## Retrieve and load the fMRI data from the Haxby study\n",
    "\n",
    "The `nilearn.datasets.fetch_haxby` function will download the\n",
    "Haxby dataset if it's not present on the disk. It'll put this in the `nilearn` data directory and only needs to be downloaded once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nilearn import datasets\n",
    "from nilearn import plotting\n",
    "\n",
    "# By default the 2nd subject will be fetched\n",
    "haxby_dataset = datasets.fetch_haxby(data_dir=path_data)\n",
    "fmri_filename = haxby_dataset.func[0]\n",
    "\n",
    "# print basic information on the dataset\n",
    "print('First subject functional nifti images (4D) are at: %s' %\n",
    "      fmri_filename)  # 4D data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the fMRI volume for model fitting\n",
    "As we saw in the previous lecture, we can easily load volume data into python using `nibabel`. However, we generally don't want the full volume, but instead prefer to use a subset of voxels based on some preferred region of the brain (e.g. the cortical surface).\n",
    "\n",
    "Effectively, we wish to do two things:\n",
    "\n",
    "1. Extract a subset of voxels from the nifty file (masking)\n",
    "2. Reshape these voxels so that they can be used to fit a model (vectorizing)\n",
    "\n",
    "`nilearn` allows us to do this easily, turning the 4-D matrix of into an array of shape (n_samples, n_features). This allows us to use scikit-learn to do machine learning.\n",
    "\n",
    "## Masking / vectorizing our data\n",
    "We will use the `nilearn.input_data.NiftiMasker` to extract the\n",
    "fMRI data on a mask and convert it to data series.\n",
    "\n",
    "First, remember that masks are simply a 3D array in the same space as our MRI data. They are basically boolean values that say whether or not to keep each voxel. \n",
    "\n",
    "We'll load and plot it below. The mask is a mask of the Ventral Temporal streaming area coming from the Haxby study:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mask_filename = haxby_dataset.mask_vt[0]\n",
    "plotting.plot_roi(mask_filename, bg_img=haxby_dataset.anat[0],\n",
    "                  cmap='Paired')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that only a few of the voxels are being shown here. These are the voxels that the mask has extracted. Now we use the NiftiMasker.\n",
    "\n",
    "We first create a masker, giving it the options that we care\n",
    "about. Here we also use standardizing of the data, as it is often important\n",
    "for decoding. This scales the data so that its mean / variance is more consistent across voxels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nilearn.input_data import NiftiMasker\n",
    "masker = NiftiMasker(mask_img=mask_filename, standardize=True)\n",
    "masker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A note on transformers**\n",
    "\n",
    "The `masker` is an object that can perform data masking for us. It follows a very similar API to `scikit-learn`. Objects that transform other objects are called `transformers`, and generally have a `transform` method on top of the `fit` method that we covered earlier.\n",
    "\n",
    "We'll `fit` the masker on our mask, then use it to vectorize our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Fitting the transformer initializes it to operate on new data\n",
    "masker.fit(fmri_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Now we'll transform our fMRI data\n",
    "fmri_masked = masker.transform(fmri_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variable \"fmri_masked\" is a numpy array. It is 2-D."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(fmri_masked)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Its shape corresponds to the number of time-points x the number of\n",
    "voxels in the mask. Note that this is much fewer than the total number of voxels in the nifty image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(fmri_masked.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the behavioral labels\n",
    "\n",
    "Now that we have our vectorized fMRI activity, we need labels for the state of the experiment in order to fit our classifier. The behavioral labels are stored in a CSV file, separated by spaces.\n",
    "\n",
    "We use `pandas` to load them in an array. This is a library that is excellent for representing and manipulating tabular data. It's got a steep learning curve but is very powerful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load target information as string and give a numerical identifier to each\n",
    "labels = pd.read_csv(haxby_dataset.session_target[0], delimiter=' ')\n",
    "print(labels.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like labels has the same length as our fMRI data, meaning that they share the same time-base."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(labels.shape)\n",
    "print(fmri_masked.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll retrieve the behavioral targets from the labels. These will be the \"classes\" that we attempt to predict.\n",
    "\n",
    "Note that these labels aren't integers like before. That's fine - `sklearn` will try to be clever and convert these into integer representations when we fit the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(labels['labels'].values[:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Restrict the analysis to cats and faces\n",
    "\n",
    "As we can see from the targets above, the experiment contains many\n",
    "conditions. Today we'll restrict the decoding to two categories of interest: cats and faces.\n",
    "\n",
    "To do this, we'll use `pandas` to create a mask corresponding to these categories, and then use it to extract only the rows we care about."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create a mask w/ Pandas\n",
    "condition_mask = labels.eval('labels in [\"face\", \"cat\"]').values\n",
    "\n",
    "# Create a mask w/ Numpy\n",
    "# condition_mask = np.logical_or(target == b'face', target == b'cat')\n",
    "\n",
    "# We apply this mask in the sample direction to restrict the\n",
    "# classification to the face vs cat discrimination\n",
    "fmri_masked = fmri_masked[condition_mask]\n",
    "targets = labels[condition_mask]['labels'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we now have fewer samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(fmri_masked.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decoding with an SVM\n",
    "\n",
    "Now we have all the components we'll need to fit a model. We have:\n",
    "\n",
    "* Masked a subset of voxels in which we are interested.\n",
    "* Vectorized those masked voxels so that they have shape (n_samples, n_features)\n",
    "* Extracted a set of labels, one for each timepoint, corresponding to the stimulus being shown at that time.\n",
    "* Masked our time dimension so that we only have two classes of interest.\n",
    "\n",
    "Now we'll fit our model. As before, we'll use the [scikit-learn](http://www.scikit-learn.org>)  toolbox on the fmri_masked data.\n",
    "\n",
    "As a decoder, we'll use a Support Vector Classification, with a linear\n",
    "kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "svc = SVC(kernel='linear')\n",
    "print(svc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As our data is already in the shape for `sklearn`, it's quite easy to fit the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "svc.fit(fmri_masked, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "prediction = svc.predict(fmri_masked)\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scoring our model\n",
    "`sklearn` has a number of functions for defining the \"score\" of a model. The proper one to use depends on the nature of your model and data. Here we'll use a simple scorer for classification. These functions expect two arrays:\n",
    "\n",
    "* The array of \"true\" classes for each sample\n",
    "* The array of predicted classes given our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print(accuracy_score(targets, prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow, a score of 100%! Ship it off to *Nature*, right? Not quite yet. As we've mentioned before, you should **always make predictions on data that hasn't been used to fit the model**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validating our model\n",
    "\n",
    "The proper way to measure error rates or prediction accuracy is via\n",
    "cross-validation: leaving out some data and testing on it. There are many ways to do this.\n",
    "\n",
    "### ...by manually leaving out data\n",
    "\n",
    "Let's leave out the 30 last data points during training, and test the\n",
    "prediction on these 30 last points:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "svc.fit(fmri_masked[:-30], targets[:-30])\n",
    "\n",
    "prediction = svc.predict(fmri_masked[-30:])\n",
    "print((prediction == targets[-30:]).sum() / float(len(targets[-30:])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, this seems unfortunate. We've now got 50% less data in order to fit the model. Ideally, we'd like to do two things:\n",
    "\n",
    "* Validate our model properly (aka, on held-out data not used in model fitting)\n",
    "* Use as much of our data as possible.\n",
    "\n",
    "It is difficult to satisfy both of these conditions properly, but *cross-validation* is one way of getting closer to this goal. \n",
    "\n",
    "### Cross-validation: Implementing a KFold loop\n",
    "\n",
    "We can split the data in train and test set repetitively in a `KFold`\n",
    "strategy. We'll create many cross-validation objects with `sklearn`. When one iterates through these, it returns different indices for training / test sets upon teach iteration.\n",
    "\n",
    "Let's visualize what this will look like below..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create the KFold object\n",
    "from sklearn.model_selection import KFold\n",
    "n_splits = 15\n",
    "cv = KFold(n_splits=n_splits)\n",
    "\n",
    "cv_sample = np.zeros([len(fmri_masked), n_splits])\n",
    "for ii, (tr,tt) in enumerate(cv.split(fmri_masked)):\n",
    "    cv_sample[tt, ii] = 1\n",
    "fig, ax = plt.subplots()\n",
    "ax.imshow(cv_sample, cmap='coolwarm', aspect='auto', interpolation='nearest')\n",
    "_ = ax.set(xlabel='Iteration', ylabel='Data Index',\n",
    "           title='Cross Validation indices\\n(red = Test set)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, on each iteration we hold out a different subset of samples. Next, we'll loop through this object, fit a model on one subset of data, and then test it on the other subset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for train, test in cv.split(fmri_masked):\n",
    "    svc.fit(fmri_masked[train], targets[train])\n",
    "    prediction = svc.predict(fmri_masked[test])\n",
    "    print(accuracy_score(targets[test], prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If all we want to do is score this model, note that `sklearn` has tools to perform cross-validation more succinctly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import cross_val_score\n",
    "cv_score = cross_val_score(svc, fmri_masked, targets)\n",
    "print(cv_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note that we can speed things up to use all the CPUs of our computer\n",
    "with the n_jobs parameter. However, be careful in doing this on a cluster environment as you may be asking for resources not available to you.\n",
    "\n",
    "By default, cross_val_score uses a 3-fold KFold. We can control this by\n",
    "passing the \"cv\" object, here a 5-fold:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cv_score = cross_val_score(svc, fmri_masked, targets,\n",
    "                           cv=cv.split(fmri_masked))\n",
    "print(cv_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's often useful to visualize these as a histogram, to get an idea for the distribution of cross-validated scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_classifier_scores(scores):\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.hist(scores)\n",
    "    ax.axvline(.5, ls='--', c='r')\n",
    "    ax.set_xlabel('Model Score')\n",
    "plot_classifier_scores(cv_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A quick note on cross-validating with time\n",
    "As neuroscientists, our data is often collected across time. This might be across a very short time-scale (milliseconds) or a long one (hours, or days). Either way, it is crucial to consider the relationships between datapoints as a function of time. Consider the following facts:\n",
    "\n",
    "* All time series data is correlated with itself (autocorrelated) to some degree\n",
    "* Confounding variables may be the same on one day of acquisition, and different on another day\n",
    "* The brain may have a different baseline internal state at the beginning of an experiment compared to the end.\n",
    "\n",
    "As we've mentioned before, you must **always test the model on \"new\" data**. This means that the training and test sets should share as *little information as possible*. In other words, anything in the training set that could give you information about the test set, but that is not related to the features of interest, will **bias** the model towards the wrong answer, or inflate your model score.\n",
    "\n",
    "\n",
    "## Leaving out recording sessions\n",
    "The best way to do cross-validation is to respect the structure of\n",
    "the experiment, for instance by leaving out full sessions of\n",
    "acquisition.\n",
    "\n",
    "The number of the session is stored in the CSV file with our\n",
    "behavioral data. We'll apply our condition mask, and then leave out one session at a time. To do this, we'll use a LeaveOneLabelOut object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Find out list of session number\n",
    "session_label = labels['chunks'][condition_mask]\n",
    "\n",
    "# Iterate through sessions, validating on a held-out session\n",
    "from sklearn.cross_validation import LeaveOneLabelOut\n",
    "cv = LeaveOneLabelOut(session_label)\n",
    "cv_score = cross_val_score(svc, fmri_masked, targets, cv=cv)\n",
    "plot_classifier_scores(cv_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspecting the model weights\n",
    "\n",
    "\n",
    "Finally, now that our model has been fit to data, and validated on held-out data, it may be useful to inspect and display the model weights. This is often used to understand the voxels that were particularly important in discriminating these two classes.\n",
    "\n",
    "`sklearn` models that are linear store their weights in an attribute called `coef_`. We'll look at this below. Note that there is one weight per feature (in this case, voxels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "coef_ = svc.coef_\n",
    "print(coef_[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(svc.coef_.shape)\n",
    "print(fmri_masked.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Turning the weights into a nifti image\n",
    "Using our `NiftiMasker`, we can collect these weights and reshape them back into the 3-D space of our fMRI data.\n",
    "\n",
    "We need to turn it back into a Nifti image, in essence, \"inverting\"\n",
    "what the NiftiMasker has done.\n",
    "\n",
    "For this, we can call inverse_transform on the NiftiMasker:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "coef_img = masker.inverse_transform(coef_)\n",
    "print(coef_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "coef_img is now a NiftiImage. Essentially, this is like the statistical \"t-maps\" that we've visualized before. \n",
    "\n",
    "We can save the coefficients as a nii.gz file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "coef_img.to_filename('haxby_svc_weights.nii.gz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing our weights\n",
    "\n",
    "Finally, we can plot the weights that the model found, using the subject's anatomical as a background."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nilearn.plotting import plot_stat_map, show\n",
    "\n",
    "plot_stat_map(coef_img, bg_img=haxby_dataset.anat[0],\n",
    "              title=\"SVM weights\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recap\n",
    "Above we've shown a sample pipeline for fitting a linear classifier using fMRI activity and object classes. We've covered a few of the basics in the machine learning pipeline, and shown how `nilearn` and `sklearn` complement one another.\n",
    "\n",
    "Much more information about these functions, and the options available for machine learning, can be found in the `nilearn` documentation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
