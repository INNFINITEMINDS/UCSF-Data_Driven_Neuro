{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to machine learning using scikit-learn\n",
    "**Credits**\n",
    "\n",
    "Code source: Gaël Varoquaux, Andreas Müller \n",
    "             \n",
    "Modified by: Chris Holdgraf\n",
    "\n",
    "License: BSD 3 clause"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import make_moons, make_circles, make_classification\n",
    "\n",
    "cols_cmap = ['#FF0000', '#0000FF']\n",
    "cmap = ListedColormap(cols_cmap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to use this guide\n",
    "This is meant to be a hybrid lecture / tutorial on the basics of machine learning, using the scikit-learn API. It's not comprehensive nor is it domain-specific, but it should give you a flavor of what's to come in the world of machine learning with python.\n",
    "\n",
    "# Principles of machine learning\n",
    "While there are many packages for performing machine learning across a range of languages, the same principles always apply. This will be a short introduction to some of these principles using an example that is on the scikit-learn tutorials section.\n",
    "\n",
    "Machine learning essentially has four main components to it:\n",
    "\n",
    "1. Feature extraction\n",
    "2. Model fitting\n",
    "3. Model validation\n",
    "4. Model inspection\n",
    "\n",
    "In python, it is easy to perform all of these steps (to some degree of satisfaction) with scikit-learn. Let's take a look at all of these steps in the context of an actual dataset.\n",
    "\n",
    "# Feature extraction\n",
    "For this step we'll use data from a canonical dataset, the \"iris classification\" dataset. In a nutshell, this consists of information collected about three different types of irises. The question then arises: can we use this information to distinguish (classify) one type of iris from another?\n",
    "\n",
    "## Collect your data\n",
    "In our case, we'll use some datasets that are packaged with scikit-learn. That way we can focus on choosing features rather than collecting data.\n",
    "\n",
    "Let's take a quick look at the data. In scikit-learn, it is quite easy to load some common datasets like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()\n",
    "iris.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameterize your data\n",
    "Parameterizing data basically comes down to deciding how you're going to describe your data. Ideally with something quantifiable. This will determine the kinds of models that we can fit.\n",
    "\n",
    "Inside this dataset is a dictionary with information about the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# These datasets come with a quick description\n",
    "print(iris['DESCR'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll focus on the `data` field, which tells gives us the feature values for now. We'll also focus on the `target` field, which tells us which class each datapoint belongs to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We can see that \"data\" consists of an array\n",
    "# It is shape (n_samples, n_features)\n",
    "print(iris['data'][[0, 1, 2, -3, -2, -1], :])\n",
    "print('---')\n",
    "print(iris['target'][[0, 1, 2, -3, -2, -1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see that we have:\n",
    "* Four features (one for each column)\n",
    "* Three classes AKA targets (an integer for each class type, don't forget python counting begins as 0)\n",
    "\n",
    "Let's choose the first and third features for visualizing the data, and we'll only use targets 1 and 3 to keep things simple\n",
    "\n",
    "## Vizualize your features\n",
    "It's always a good idea to visualize your raw data before you do any fancy model fitting or statistical calculations. This lets you see if the data \"makes sense\" visually and can spot bugs early on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Return a boolean mask for the targets we care about\n",
    "targets = [0, 2]\n",
    "mask_samples = np.array([ii in targets\n",
    "                         for ii in iris['target']], dtype=bool)\n",
    "\n",
    "# These are the features we'll use\n",
    "features = [0, 1]\n",
    "name1, name2 = [iris['feature_names'][ii] for ii in targets]\n",
    "\n",
    "# Now pull the datapoints\n",
    "X = iris['data'][:, features][mask_samples]\n",
    "y = iris['target'][mask_samples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.scatter(X[:, 0], X[:, 1], c=y, cmap=cmap, s=100)\n",
    "ax.set(xlabel=name1, ylabel=name2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, we've defined two sets of features of interest for our data, along with two classes we'd like to predict. Because we are predicting a class that each datapoint belongs to, we call this problem a \"classification\" problem. Models that solve this problem are called \"classifiers\".\n",
    "\n",
    "# Fitting / Validating the model using `sklearn`\n",
    "Now that we've got our features defined, we need a model that can distinguish the red from the blue class. `scikit-learn` has several such models. Here we'll use a support vector machine. Its task will be to find a line between the points shown above such that the distance from the line is far away from the center of each cluster of points.\n",
    "\n",
    "## The `scikit-learn` model syntax\n",
    "Note that many models take a parameter when you instantiate them. This is often a free parameter that is model-specific. In the case of support vector machines, the parameter controls how strict the model is about finding allowing for incorrectly-labeled datapoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "svc = SVC(C=1., kernel='linear')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Models in `scikit-learn` all use the same basic API. They have a `fit` method, which takes as input either one or two arguments corresponding to the data that should be used to fit the model. Many of them also have a `predict` method that will output a model prediction for a new input point. If the model is a classifier, the prediction will be a label (generally an integer) that corresponds to a class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Fit the model with the given features / classes\n",
    "svc.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating predictions with our model\n",
    "Now we have a fit model, which can be used to make predictions about new data. We'll use the `predict` method to accomplish this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Now we can use this model to predict the class of new data\n",
    "svc.predict(X[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validating our model\n",
    "As we mentioned before, it is important to **validate** the model as well. Good machine learning practice says that we *cannot* test the model on the same data that we used to fit it. So, we'll split up the data into a training and a testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, KFold, ShuffleSplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = \\\n",
    "        train_test_split(X, y, test_size=.4, random_state=1337)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll fit the model on our training data, and validate the model on the held out data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Training the model requires both input and output data\n",
    "svc.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predictions = svc.predict(X_test)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note how the model predicted integers, corresponding to class 0 or 2. Let's plot the model predictions along with the two input feature values to see how they look:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We'll set the training data to be white on the inside\n",
    "# Then we can compare it with the model predictions\n",
    "edgecolors = np.where(y_train == targets[0], cols_cmap[0], cols_cmap[1])\n",
    "\n",
    "# White will be the training data\n",
    "# Filled-in will be the testing data\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(X_train[:, 0], X_train[:, 1], c='w', s=200,\n",
    "           alpha=.2, edgecolors=edgecolors)\n",
    "ax.scatter(X_test[:, 0], X_test[:, 1], c=predictions, s=100, cmap=cmap)\n",
    "ax.set(xlabel=name1, ylabel=name2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model inspection\n",
    "As we can see, the model did a great job of separating these two classes. That's because they're easily separated by a single line drawn between the two groups. In machine learning terms, they are \"linearly separable\".\n",
    "\n",
    "## Exploring model coefficients\n",
    "We can look at the coefficients of the model to understand how it learned to used the features in order to predict a class. Coefficients are generally created at the `coef_` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# There is one coefficient for each feature\n",
    "svc.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot coefficient values\n",
    "If we have relatively few features, then we can visualize the line that the model has learned. It is using this line to decide if a new datapoint is of class 0 or 2. Let's see what this line looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Remake the scatterplot\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(X_train[:, 0], X_train[:, 1], c='w', s=200,\n",
    "           alpha=.2, edgecolors=edgecolors)\n",
    "ax.scatter(X_test[:, 0], X_test[:, 1], c=predictions, s=100, cmap=cmap)\n",
    "\n",
    "# Now plot the shape of the dividing hyperplane\n",
    "plt_x = np.linspace(*ax.get_xlim(), num=100)\n",
    "w = svc.coef_[0]\n",
    "a = -w[0] / w[1]\n",
    "line_y = a * plt_x - svc.intercept_[0] / w[1]\n",
    "\n",
    "# plot the parallels to the separating hyperplane that pass through the\n",
    "# support vectors\n",
    "b = svc.support_vectors_[0]\n",
    "yy_down = a * plt_x + (b[1] - a * b[0])\n",
    "\n",
    "b = svc.support_vectors_[-1]\n",
    "yy_up = a * plt_x + (b[1] - a * b[0])\n",
    "\n",
    "# plot the line, the points, and the nearest vectors to the plane\n",
    "ax.plot(plt_x, line_y, 'k')\n",
    "ax.plot(plt_x, yy_down, 'k--')\n",
    "ax.plot(plt_x, yy_up, 'k--')\n",
    "plt.autoscale(tight=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the line does a pretty good job of  splitting the two groups. This is the structure that the model has learned in the data. If you give it a new datapoint, it will determine which side of the line that datapoint lies on, and will choose the class belonging to that side of the line.\n",
    "\n",
    "What can we learn from this line? It's hard to determine, and highly question-specific. However, especially in the case of multiple input features, it may be possible to use the coefficients the model found in order to assume which features are \"important\" in distinguishing between classes.\n",
    "\n",
    "# Recap\n",
    "This has been a *really* quick introduction to the core principles of machine learning. We've used classification here as an example, and there are many other kinds of models that one can fit in order to understand the brain. In the following lectures, we'll cover a package that is built on top of scikit-learn and is specialized for neuroimaging data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra: Custom Cross-Validation iterators\n",
    "It's possible to define your own cross-validation iterator if you need multiple iterations for fitting the model. Below we'll show how to create an iterator, and loop through training / test indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Classes like `ShuffleSplit` randomly shuffle all the data on each split\n",
    "cv1 = ShuffleSplit(n_splits=10, test_size=.2)\n",
    "label1 = 'Shuffle Split'\n",
    "\n",
    "# Classes like `KFold` let you split the data in structured subsets\n",
    "cv2 = KFold(n_splits=10)\n",
    "label2 = 'KFold'\n",
    "\n",
    "for cv, label in [(cv1, label1), (cv2, label2)]:\n",
    "    fig, ax = plt.subplots()\n",
    "    for ii, (tr, tt) in enumerate(cv.split(X)):\n",
    "        ax.plot(np.tile(ii, [tr.shape[0]]), tr, 'or', ms=4)\n",
    "        ax.plot(np.tile(ii, [tt.shape[0]]), tt, 'ob', ms=4)\n",
    "    ax.set_xlabel('Iteration')\n",
    "    ax.set_ylabel('Index')\n",
    "    ax.set_title(label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra: Comparing lots of classifiers\n",
    "As we mentioned, there are a *lot* of classifiers in machine learning (and in scikit-learn). Below we'll plot the performance of a number of classifiers for several datasets.\n",
    "\n",
    "For each dataset / classifier, we'll show the raw datapoints, and color each datapoint according to its class.\n",
    "\n",
    "Then, we'll fit each classifier on the data and make predictions about the label for a range of values in the input space. This gives us an idea for which parts of the input space the classifier thinks \"belong\" to a given class. This is similar to plotting the line above, though it's a more empirically-driven way of visualizing the same idea."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "\n",
    "names = [\"Nearest Neighbors\", \"Linear SVM\", \"RBF SVM\", \"Gaussian Process\",\n",
    "         \"Decision Tree\", \"Random Forest\", \"Neural Net\", \"AdaBoost\",\n",
    "         \"Naive Bayes\", \"QDA\"]\n",
    "\n",
    "classifiers = [\n",
    "    KNeighborsClassifier(3),\n",
    "    SVC(kernel=\"linear\", C=0.025),\n",
    "    SVC(gamma=2, C=1),\n",
    "    GaussianProcessClassifier(1.0 * RBF(1.0), warm_start=True),\n",
    "    DecisionTreeClassifier(max_depth=5),\n",
    "    RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),\n",
    "    MLPClassifier(alpha=1),\n",
    "    AdaBoostClassifier(),\n",
    "    GaussianNB(),\n",
    "    QuadraticDiscriminantAnalysis()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use a few functions scikit-learn uses for creating fake datasets. This will help visualize the behavior of each classifier for a different pattern of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# Collect some extra parameters for this viz\n",
    "h = .02  # step size in the mesh\n",
    "X_plt, y_plt = X.copy(), y.copy()  # So we don't overwrite these\n",
    "linearly_separable = (X_plt, y_plt)\n",
    "\n",
    "# Define the datasets we'll use (we'll loop through these)\n",
    "datasets = [make_moons(noise=0.3, random_state=0),\n",
    "            make_circles(noise=0.2, factor=0.5, random_state=1),\n",
    "            linearly_separable\n",
    "            ]\n",
    "\n",
    "# Create the figure and then populate it with data\n",
    "fig, axs = plt.subplots(len(classifiers), len(datasets), figsize=(9, 27))\n",
    "# iterate over datasets\n",
    "for ii, ds in enumerate(datasets):\n",
    "    # preprocess dataset, split into training and test part\n",
    "    X, y = ds\n",
    "    X = StandardScaler().fit_transform(X)\n",
    "    X_train, X_test, y_train, y_test = \\\n",
    "        train_test_split(X, y, test_size=.4, random_state=42)\n",
    "\n",
    "    x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\n",
    "    y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "\n",
    "    # just plot the dataset first\n",
    "    cm = plt.cm.RdBu\n",
    "    for ax in axs[:, ii]:\n",
    "        if ii == 0:\n",
    "            ax.set_title(\"Input data\")\n",
    "        # Plot the training points\n",
    "        ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cmap)\n",
    "        # and testing points\n",
    "        ax.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=cmap, alpha=0.6)\n",
    "        ax.set_xlim(xx.min(), xx.max())\n",
    "        ax.set_ylim(yy.min(), yy.max())\n",
    "        ax.set_xticks(())\n",
    "        ax.set_yticks(())\n",
    "\n",
    "    # iterate over classifiers\n",
    "    for jj, (name, clf) in enumerate(zip(names, classifiers)):\n",
    "        ax = axs[jj, ii]\n",
    "        clf.fit(X_train, y_train)\n",
    "        score = clf.score(X_test, y_test)\n",
    "\n",
    "        # Plot the decision boundary. For that, we will assign a color to each\n",
    "        # point in the mesh [x_min, x_max]x[y_min, y_max].\n",
    "        if hasattr(clf, \"decision_function\"):\n",
    "            Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])\n",
    "        else:\n",
    "            Z = clf.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, 1]\n",
    "\n",
    "        # Put the result into a color plot\n",
    "        Z = Z.reshape(xx.shape)\n",
    "        ax.contourf(xx, yy, Z, cmap=cm, alpha=.8)\n",
    "\n",
    "        # Plot also the training points\n",
    "        ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cmap)\n",
    "        # and testing points\n",
    "        ax.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=cmap,\n",
    "                   alpha=0.6)\n",
    "\n",
    "        ax.set_xlim(xx.min(), xx.max())\n",
    "        ax.set_ylim(yy.min(), yy.max())\n",
    "        ax.set_xticks(())\n",
    "        ax.set_yticks(())\n",
    "        if ii == 0:\n",
    "            ax.set_title(name)\n",
    "        ax.text(xx.max() - .3, yy.min() + .3, ('%.2f' % score).lstrip('0'),\n",
    "                size=15, horizontalalignment='right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
