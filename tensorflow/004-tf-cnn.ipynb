{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_and_decode(filename):\n",
    "    filename_queue = tf.train.string_input_producer([filename],\n",
    "                                                    num_epochs=None)\n",
    "\n",
    "    reader = tf.TFRecordReader()\n",
    "    _, serialized_example = reader.read(filename_queue)\n",
    "    features = tf.parse_single_example(\n",
    "        serialized_example,\n",
    "        features={\n",
    "                'image/height': tf.FixedLenFeature([], tf.int64),\n",
    "                'image/width': tf.FixedLenFeature([], tf.int64),\n",
    "                'image/depth': tf.FixedLenFeature([], tf.int64),\n",
    "                'label': tf.FixedLenFeature([], tf.int64),\n",
    "                'image/raw': tf.VarLenFeature(tf.string)})\n",
    "\n",
    "    # Shape elements must be int32 tensors!\n",
    "    height = tf.cast(features['image/height'], tf.int32)\n",
    "    width = tf.cast(features['image/width'], tf.int32)\n",
    "    depth = tf.cast(features['image/depth'], tf.int32)\n",
    "    \n",
    "    # Decode the image from its raw representation:\n",
    "    image = tf.decode_raw(features['image/raw'].values, tf.uint8)\n",
    "\n",
    "    # Reshape it back to its original shape:\n",
    "    im_shape = tf.pack([height, width, depth])\n",
    "    image = tf.reshape(image, im_shape)\n",
    "    #tf.random_crop(image, [height, width, depth])\n",
    "    # Convert from [0, 255] -> [0, 1] floats.\n",
    "    image = tf.cast(image, tf.float32) * (1. / 255)\n",
    "\n",
    "    # Convert label from a scalar uint8 tensor to an int32 scalar.\n",
    "    label = tf.cast(features['label'], tf.int32)\n",
    "    return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "image, label = read_and_decode(\"../data/cells_train.tfrecords\")\n",
    "sess = tf.Session()\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "tf.train.start_queue_runners(sess=sess)\n",
    "im_1, lab_1 = sess.run([image, label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "image, label = read_and_decode(\"../data/cells_train.tfrecords\")\n",
    "images_batch, labels_batch = tf.train.shuffle_batch(\n",
    "    [image, label], batch_size=10,\n",
    "    capacity=400,\n",
    "    shapes=(im_1.shape, lab_1.shape),\n",
    "    min_after_dequeue=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def weight_variable(shape, fan_in, fan_out):\n",
    "    \"\"\" \n",
    "    Initialize weights with the Xavier initialization\n",
    "    \"\"\"\n",
    "    low = -4*np.sqrt(6.0/(fan_in + fan_out)) \n",
    "    high = 4*np.sqrt(6.0/(fan_in + fan_out))\n",
    "    return tf.Variable(tf.random_uniform(shape, minval=low, maxval=high, dtype=tf.float32))\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv2d(x, W):\n",
    "    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],\n",
    "                          strides=[1, 2, 2, 1], padding='SAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "W_conv1 = weight_variable([5, 5, 3, 32], 256*256*3, 32)\n",
    "b_conv1 = bias_variable([32])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First convolutional layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "h_conv1 = tf.nn.relu(conv2d(images_batch, W_conv1) + b_conv1)\n",
    "h_pool1 = max_pool_2x2(h_conv1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second convolutional layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W_conv2 = weight_variable([5, 5, 32, 64], 128*128*32, 64)\n",
    "b_conv2 = bias_variable([64])\n",
    "\n",
    "h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\n",
    "h_pool2 = max_pool_2x2(h_conv2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fully connected layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(10), Dimension(64), Dimension(64), Dimension(64)])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h_pool2.get_shape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "W_fc1 = weight_variable([64 * 64 * 64, 1024], 64*64*64, 1024)\n",
    "b_fc1 = bias_variable([1024])\n",
    "\n",
    "h_pool2_flat = tf.reshape(h_pool2, [-1, 64 * 64 * 64])\n",
    "h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropout:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "keep_prob = tf.placeholder(tf.float32)\n",
    "h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Readout:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W_fc2 = weight_variable([1024, 3], 1024, 3)\n",
    "b_fc2 = bias_variable([3])\n",
    "y_conv = tf.matmul(h_fc1_drop, W_fc2) + b_fc2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss = tf.nn.sparse_softmax_cross_entropy_with_logits(y_conv, labels_batch)\n",
    "loss_mean = tf.reduce_mean(loss)\n",
    "train_op = tf.train.AdamOptimizer(1e-5).minimize(loss_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred = tf.cast(tf.argmax(y_conv, 1), tf.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "correct_prediction = tf.equal(y_pred, labels_batch)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # These variables are used for evaluation (helping to decide when to stop training):\n",
    "# image_eval, label_eval = read_and_decode(\"../data/cells_eval.tfrecords\")\n",
    "\n",
    "# this_image = tf.placeholder(\"float\", shape=[1, im_1.shape[0], im_1.shape[1], im_1.shape[2]])\n",
    "# this_label = tf.placeholder(\"int32\", shape=[])\n",
    "\n",
    "# # Reproducing the entire network on eval data:\n",
    "# h_conv1_eval = tf.nn.relu(conv2d(this_image, W_conv1) + b_conv1)\n",
    "# h_pool1_eval = max_pool_2x2(h_conv1_eval)\n",
    "\n",
    "# h_conv2_eval = tf.nn.relu(conv2d(h_pool1_eval, W_conv2) + b_conv2)\n",
    "# h_pool2_eval = max_pool_2x2(h_conv2)\n",
    "\n",
    "# h_pool2_flat_eval = tf.reshape(h_pool2, [-1, 64 * 64 * 64])\n",
    "# h_fc1_eval = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\n",
    "\n",
    "# y_pred_eval = tf.matmul(h_fc1_eval, W_fc2) + b_fc2\n",
    "\n",
    "# correct_prediction = tf.reduce_mean(\n",
    "#     tf.cast(tf.equal(tf.cast(tf.argmax(y_pred_eval, 1), tf.int32), this_label), tf.int8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<Thread(Thread-4, started daemon 123145547661312)>,\n",
       " <Thread(Thread-5, started daemon 123145552916480)>,\n",
       " <Thread(Thread-6, started daemon 123145558171648)>]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "tf.train.start_queue_runners(sess=sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, training accuracy 0.1\n",
      "4.59583\n",
      "97.8362\n",
      "50.8063\n",
      "45.7374\n",
      "54.0524\n",
      "27.2078\n",
      "2.7953\n",
      "33.5259\n",
      "4.95413\n",
      "15.3977\n",
      "step 10, training accuracy 0.4\n",
      "5.93069\n",
      "3.9617\n",
      "4.18758\n",
      "5.09049\n",
      "3.07566\n",
      "2.73532\n",
      "1.85238\n",
      "1.75176\n",
      "2.1985\n",
      "1.66059\n",
      "step 20, training accuracy 0.3\n",
      "1.02634\n",
      "1.31306\n",
      "1.49257\n",
      "0.999313\n",
      "1.2427\n",
      "1.04478\n",
      "1.0548\n",
      "0.988771\n",
      "0.996578\n",
      "0.932116\n",
      "step 30, training accuracy 0.1\n",
      "1.01739\n",
      "1.19258\n",
      "0.961293\n",
      "1.04729\n",
      "1.07643\n",
      "0.727236\n",
      "0.861332\n",
      "0.839987\n",
      "1.37751\n",
      "1.84061\n",
      "step 40, training accuracy 0.4\n",
      "1.45401\n",
      "1.32185\n",
      "1.02408\n",
      "0.88612\n",
      "1.24057\n",
      "1.35262\n",
      "1.10012\n",
      "1.32042\n",
      "1.03975\n",
      "1.2472\n",
      "step 50, training accuracy 0.6\n",
      "1.33491\n",
      "1.23112\n",
      "1.2214\n",
      "1.28937\n",
      "1.03781\n",
      "1.10638\n",
      "1.36075\n",
      "1.07257\n",
      "1.38713\n",
      "1.0697\n",
      "step 60, training accuracy 0.6\n",
      "1.12484\n",
      "1.21361\n",
      "1.20904\n",
      "1.20397\n",
      "0.877946\n",
      "1.28786\n",
      "1.14204\n",
      "1.29382\n",
      "1.02098\n",
      "1.19843\n",
      "step 70, training accuracy 0.5\n",
      "1.39144\n",
      "1.13416\n",
      "1.16539\n",
      "1.063\n",
      "1.07501\n",
      "1.5122\n",
      "1.28589\n",
      "1.32029\n",
      "1.1073\n",
      "1.08224\n",
      "step 80, training accuracy 0.3\n",
      "1.06803\n",
      "0.95657\n",
      "1.11021\n",
      "1.13795\n",
      "1.35367\n",
      "1.23982\n",
      "0.73891\n",
      "1.19062\n",
      "1.11327\n",
      "1.44782\n",
      "step 90, training accuracy 0.4\n",
      "1.21972\n",
      "1.15299\n",
      "0.979909\n",
      "1.18737\n",
      "1.05646\n",
      "1.22871\n",
      "1.11189\n",
      "1.11723\n",
      "1.05141\n",
      "1.45551\n"
     ]
    }
   ],
   "source": [
    "mean_losses = []\n",
    "for i in range(100):\n",
    "    # Every 10 steps, evaluate accuracy:\n",
    "    if i%10 == 0:\n",
    "        train_accuracy = sess.run(accuracy, feed_dict={keep_prob: 1.0})\n",
    "        print(\"step %d, training accuracy %g\" % (i, train_accuracy)) \n",
    "\n",
    "    _, loss_mean_val = sess.run([train_op, loss_mean], feed_dict={keep_prob: 0.5})\n",
    "    mean_losses.append(loss_mean_val)\n",
    "    print(loss_mean_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.40256444,  0.66997373,  1.46841836,  1.0090766 ,  1.04607761,\n",
       "        1.53120852,  0.88761938,  1.02302587,  1.49911809,  1.01013064], dtype=float32)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sess.run(y_pred, feed_dict={keep_prob: 0.5})\n",
    "sess.run(loss, feed_dict={keep_prob: 1.0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# n_iterations = 0\n",
    "# mean_losses = []\n",
    "# mean_evals = []\n",
    "# max_iterations = 5000\n",
    "\n",
    "# while True:    \n",
    "#     # We need to feed it the drop-out probability from a floating point variable\n",
    "#     _, loss_mean_val = sess.run([train_op, loss_mean], feed_dict={keep_prob: 0.5})\n",
    "#     # Store the loss so we can look at it:\n",
    "#     mean_losses.append(loss_mean_val)\n",
    "#     print(loss_mean_val)\n",
    "#     # Every 10 learning iterations, we consider whether to stop:\n",
    "#     if np.mod(n_iterations, 10) == 0:\n",
    "#         p = []\n",
    "#         # We're hard-coding the size of the entire eval data here and predicting them one by one:\n",
    "#         for ii in range(169):\n",
    "#             im, la = sess.run([image_eval, label_eval])\n",
    "#             # It expects a 4D input, because that's what TF convs take as input:\n",
    "#             im = im[np.newaxis, :]\n",
    "#             p.append(sess.run(correct_prediction, feed_dict={this_image:im, this_label:la}))\n",
    "\n",
    "#         mean_evals.append(np.mean(p))\n",
    "#         print(\"After %s iterations, cross-validation accuracy is: %s\" % (n_iterations, mean_evals[-1]))\n",
    "#         # But we really only start thinking about stopping \n",
    "#         # after 2000 iterations:\n",
    "#         if n_iterations > 2000:\n",
    "#             # Here's how we decide whether to keep going, \n",
    "#             # based on the held-out data:            \n",
    "#             # If you are still improving, relative to recent past keep training:\n",
    "#             if mean_evals[-1] < (np.mean(mean_evals[-10:-1])):\n",
    "#                 break\n",
    "\n",
    "#     # If we're still around iterate:\n",
    "#     n_iterations = n_iterations + 1  \n",
    "\n",
    "#     # If you kept going for very long, break anyway:\n",
    "#     if n_iterations > max_iterations:\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'n_iterations' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-6cf814362c49>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmean_losses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0max2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtwinx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0max2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_iterations\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean_evals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ro'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'n_iterations' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEACAYAAACpoOGTAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzt3XuUnHWd5/H3J51b59YJYBJDCLdACOE+EG66NgJyFTiO\nIoFxRRyXcxaUM86MgJ6R1jMz4sxxBlzQ2ewgm/XIosLuEgRNjNCKM8NNIIHcIZIbpAOEXMmlL9/9\n41dFV5qq7k5Xd9dT3Z/XOc/prqee+j1PPVX1fOp3eZ5SRGBmZtbRkEpvgJmZZZMDwszMinJAmJlZ\nUQ4IMzMrygFhZmZFOSDMzKyoLgNC0n2SmiQtKZg3QdJCSSslLZBUV3Df7ZJWS1ou6RN9teFmZpYU\nO04XWeb7uWPzS5JO6U653alB3A9c1GHebcCiiJgBPAHcntuA44GrgZnAJcAPJKk7G2JmZj1W7Dj9\nPkmXAEdHxDHAjcC/dKfQLgMiIn4PvNth9pXAvNz/84Crcv9fATwYES0R8TqwGpjdnQ0xM7OeKXGc\nLnQl8L9yyz4D1Ema1FW5Pe2DmBgRTbmVbQIm5uYfCqwvWG5jbp6ZmVVOj47NvdVJ7et1mJkNMEN7\n+LgmSZMioknSZGBzbv5G4LCC5abm5n2AJIeKmVkPRMSB9u12+9hcqLs1COWmvPnA9bn/Pw88UjD/\nGknDJR0JTAeeLVVoRHiK4I477qj4NmRl8r7wvvC+6Hw6gON0ofnAfwaQdBawNXLdBJ3psgYh6QGg\nHjhY0jrgDuBO4OeSbgDWkkYuERHLJP0MWAY0A/81unhGZmZWnhLH6eFARMTciHhc0qWSXgV2AV/o\nTrldBkREXFvirgtKLP8d4DvdWXn7Y8CDYc3MeqaT43ThMjcfaLmZOJP6nHNgzZpKb0Xl1NfXV3oT\nMsP7op33RTvvi8pQpVqAJL3f+jRjBvzkJ3D66RXZFDOzqiGJOPBO6h7JRA2ipQV27670VpiZWaHM\nBMSePZXeCjMzK+SAMDOzohwQZmZWVCYCornZAWFmljWZCAjXIMzMsiczAeFRTGZm2ZKZgHANwsws\nWxwQZmZWVMUDoq0tXYvJAWFmli0VD4iWlvTXAWFmli0OCDMzK6riAdHcnP56FJOZWbZUPCBcgzAz\nyyYHhJmZFeWAMDOzohwQZmZWVGYCwp3UZmbZkpmAcA3CzCxbMhEQkgPCzCxrMhEQY8c6IMzMsqbi\nAdHc7IAwM8uiigdESwuMGeOAMDPLmswEhEcxmZllSyYCYvRo2LcvXfrbzMyyIRMBMWwYjBgBe/dW\nemvMzCwvEwExdCiMHOl+CDOzLMlMQNTWOiDMzLIkMwHhGoSZWbZUPCCam1MfxMiRHslkZpYlFQ8I\n1yDMzLLJAWFmZkU5IMzMrKjMBIRHMZmZZUtZASHpLyS9ImmJpJ9IGi5pgqSFklZKWiCprrMyXIMw\nMyufpIslrZC0StKtRe4fJ2m+pJckvSzp+q7K7HFASJoCfBk4LSJOAoYCc4DbgEURMQN4Ari9s3IK\nA8KjmMzMDpykIcA9wEXALGCOpOM6LHYTsDQiTgHOA74naWhn5ZbbxFQDjM6tpBbYCFwJzMvdPw+4\nqrMCXIMwMyvbbGB1RKyNiGbgQdKxuFAAY3P/jwXeiYiWzgrtcUBExBvA94B1pGDYFhGLgEkR0ZRb\nZhMwsbNyHBBmZmU7FFhfcHtDbl6he4DjJb0BLAZu6arQTqsXnZE0npRQhwPbgJ9Luo6UUoU63n5f\nQ0MDTz2VruQ6eXI9e/bU93RzzMwGpMbGRhobG3ujqIuAFyPi45KOBn4t6aSI2FnqAYooefzulKRP\nAxdFxJdytz8HnAV8HKiPiCZJk4EnI2JmkcdHRPDtb6daRGsrjBoF3/hGjzbHzGxQkEREqMO8s4CG\niLg4d/s2ICLiuwXL/AL4TkT8W+72b4BbI+L5Uusqpw9iHXCWpJGSBJwPLAPmA9fnlvk88EhnhbiJ\nycysbM8B0yUdLmk4cA3pWFxoLXABgKRJwLHAms4K7XETU0Q8K+kh4EWgOfd3Lqnz42eSbsht0NWd\nldPSksJh2DDYtq2nW2NmNnhFRKukm4GFpC/+90XEckk3prtjLvC3wP+UtCT3sK9FxJbOyu1xE1O5\n8k1MX/saHHJIOlFu5Uq4556KbI6ZWVUo1sTUVzJzJrWbmMzMssUBYWZmRWUmIHwtJjOzbMlEQOR/\nMMgBYWaWHRUPiOZmX4vJzCyLKh4Q7oMwM8smB4SZmRXlgDAzs6IyExAexWRmli2ZCQjXIMzMsiVT\nAeFRTGZm2ZGpgHANwswsOyoeEM3N6US5ESNSQFTo2oFmZtZBxQMiX4MYOhRqalJgmJlZ5WUmIMAj\nmczMsiRTAeF+CDOz7MhcQHgkk5lZNmQuIFyDMDPLBgeEmZkV5YAwM7OiHBBmZlZUxQMif6IceJir\nmVmWVDwgPIrJzCybMhcQrkGYmWWDA8LMzIpyQJiZWVEOCDMzKypTAeFRTGZm2VHRgGhrS9OQ3FZ4\nFJOZWXZUNCBaWtI5EFK67SYmM7PsqHhA5JuXwAFhZpYlDggzMyvKAWFmZkVlKiA8isnMLDsyFRAe\nxWRmlh1lBYSkOkk/l7Rc0lJJZ0qaIGmhpJWSFkiqK/V4NzGZmfUOSRdLWiFplaRbSyxTL+lFSa9I\nerKrMsutQdwNPB4RM4GTgRXAbcCiiJgBPAHcXurBDggzs/JJGgLcA1wEzALmSDquwzJ1wL3A5RFx\nAvCZrsrtcUBIGgd8NCLuB4iIlojYBlwJzMstNg+4qlQZDggzs14xG1gdEWsjohl4kHQsLnQt8HBE\nbASIiLe7KrScGsSRwNuS7pf0gqS5kkYBkyKiKbcBm4CJpQoo/LEgcECYmfXQocD6gtsbcvMKHQsc\nJOlJSc9J+lxXhZYTEEOB04B7I+I0YBepeSk6LNfx9vs8isnMrN/kj9mXABcDfyNpelcP6KkNwPqI\neD53+2FSQDRJmhQRTZImA5tLFXDvvQ1s3gwNDVBfX89RR9V7FJOZWYHGxkYaGxu7WmwjMK3g9tTc\nvEIbgLcjYg+wR9LvSH3Hr5YqVBElv+B3SdJvgS9FxCpJdwCjcndtiYjv5nrSJ0TEbUUeG08/HXzl\nK/DMM2ne5s0waxa89VaPN8nMbECTRESow7waYCVwPvAm8CwwJyKWFyxzHPDfSLWHEcAzwGcjYlmp\ndZVTgwD4CvATScOANcAXgBrgZ5JuANYCV5d6sDupzczKFxGtkm4GFpK6Du6LiOWSbkx3x9yIWCFp\nAbAEaAXmdhYOUGYNohySorEx+OY34be/TfP27YPRo1PntZmZfVCxGkRfydSZ1MOGQWtrmm9mZpWV\nqYCQPJLJzCwrKhoQHc+DAPdDmJllRaZqEOCAMDPLCgeEmZkV5YAwM7OiHBBmZlZU5gKittY/GmRm\nlgWZC4jOahAvv+yT6MzM+ktVBcSnPgXPPtv322VmZlUUEDt3wmuvwbZt/bNtZmaDXdWcKLd0KUQ4\nIMzM+kvV1CCWLEl/t2/v++0yM7MMBkSpUUyLF6f7HBBmZv0jcwHRWQ3izDMdEGZm/aUqAiIiBcS5\n57oPwsysv2QuIA46CDZt2n/e+vUwahQcfbRrEGZm/SVzAXHBBfCrX6VaQ96SJXDyyVBX54AwM+sv\nmQuIE05Ivyq3fHn7vMWL4aSTYNw4NzGZmfWXigdEx/MgJLjsMnjssfZ5S5a0B4RrEGZm/aPiJ8p1\nrEEAXH45/OIX7bfzAeEmJjOz/lPxGkSxgDjvPHjxRdiyJZ0T8frrMGOGm5jMzPpTkcNz/ykVELW1\nUF8PCxbAMcfAscfC8OFuYjIz60+ZrEFAez9EfgQTpKGu+/b5kt9mZv0h0wHxy1/CCy+k/gdIHdiu\nRZiZ9Y/MBsTUqTBtGjzwQHtAgAPCzKy/ZDYgII1mevfd/QPCI5nMzPpHpgPik5+ESZPSlOcahJlZ\n/6j4eRAdT5QrNHs2vPRS6nvI81BXM7P+kekaBMDkyfvfdg3CzKx/ZD4gOnIfhJlZ/6i6gHATk5lZ\n/6jKgHANwsys71VdQLiJycysf1RdQLiJycysf1RlQLgGYWbW98oOCElDJL0gaX7u9gRJCyWtlLRA\nUl2px7qJycysd0i6WNIKSask3drJcmdIapb0qa7K7I0axC3AsoLbtwGLImIG8ARwe6kHdnWiXDGu\nQZiZ7U/SEOAe4CJgFjBH0nEllrsTWNCdcssKCElTgUuBfy2YfSUwL/f/POCqUo93H4SZWa+YDayO\niLUR0Qw8SDoWd/Rl4CFgc3cKLbcG8c/AXwNRMG9SRDQBRMQmYGKpB7uJycysVxwKrC+4vSE3732S\npgBXRcQPAdENPQ4ISZcBTRHxUhcri1J3uJPazKzf3AUU9k10GRLl/OToucAVki4FaoGxkn4MbJI0\nKSKaJE2mk6rM9u0N/NM/pV+Kq6+vp76+vsuVjhyZgmXfvvQzpGZmA1ljYyONjY1dLbYRmFZwe2pu\nXqHTgQclCTgEuERSc0TML1WoIkp+we82SR8D/jIirpD0D8A7EfHdXE/6hIi4rchjoq4ueP11GD/+\nwNZ3yCGwYkX6a2Y2mEgiItRhXg2wEjgfeBN4FpgTEctLlHE/8GhE/J/O1tUX50HcCVwoKb+xd5Za\nsCdNTOBmJjOzQhHRCtwMLASWAg9GxHJJN0r6L8Ue0p1ye6UG0ROSYsSIYOvW1Gx0IE45Be6/H049\ntW+2zcwsq4rVIPpKpn8wqBTXIMzM+l5FA6KtDYb0YAs81NXMrO9VNCCGDt3/50S7yyfLmZn1vYoH\nRE+4icnMrO9VZUC4icnMrO9VZUC4BmFm1veqNiDcB2Fm1reqMiDcxGRm1veqMiDcxGRm1vcqGhA9\nOUkOum5ieuwx2LKlZ2WbmVlSlTWIrpqY/v7v4emne1a2mZklVRkQXTUx7dzpJigzs3JVbUB01sS0\nY0eazMys56oyILpqYnINwsysfFUZECNGpL979xa/3zUIM7PyVWVAQOl+iJYW2LPHAWFmVq6qDYi6\nuuL9EDt3pr9uYjIzK09VngcBpWsQ+YBwDcLMrDxVW4MoFRD5YHANwsysPFUdEJ01MbkGYWZWnqoN\niFJDXXfs8MX8zMx6Q9UGRGd9EFOmuAZhZlauqg6IYk1MO3Y4IMzMekPVBkSpZqR8DcJNTGZm5anq\ngNi69YPzd+yAD30I2tpKn2ltZmZdq+qAKDWKaezYNLmZycys56r2RLnx40v3QYwZk/ooHBBmZj1X\ntTWI8eOLNzEV1iDcD2Fm1nMDLiBcgzAz6x1VGxClOqndB2Fm1juqNiC66oNwE5OZWXmqNiBGjYJ9\n+9JUKF+DcBOTmVl5qjYgpOJDXV2DMDPrHVUbEFC8mck1CDOz3lG150FA8ZFMrkGYmfWOqq9BFAZE\nRKpBeJirmVn5ehwQkqZKekLSUkkvS/pKbv4ESQslrZS0QFJdqTLKDYiOQ11374bhw1O5HuZqZoOJ\npIslrZC0StKtRe6/VtLi3PR7SSd2VWY5NYgW4KsRMQs4G7hJ0nHAbcCiiJgBPAHcXqqA3u6DyPc/\nQOnfizAzG2gkDQHuAS4CZgFzcsfjQmuA/xQRJwN/C/yPrsrtcUBExKaIeCn3/05gOTAVuBKYl1ts\nHnBVqTJ6u4kp3/8ArkGY2aAyG1gdEWsjohl4kHQsfl9EPB0R+a/UTwOHdlVor/RBSDoCOCW30kkR\n0ZTboE3AxFKP6+2AyPc/gDupzWxQORRYX3B7A50HwJ8Dv+yq0DIP0SBpDPAQcEtE7JQUHRbpePt9\njz7awPrcU6qvr6e+vv6A1l1XB6tXt9/esWP/JibXIMys2jU2NtLY2Nhr5Uk6D/gC8JGuli0rICQN\nJYXDjyPikdzsJkmTIqJJ0mRgc6nHf+YzDVx3Xc/XX6wPwjUIMxtIOn55/ta3vlVssY3AtILbU3Pz\n9iPpJGAucHFEvNvVusttYvoRsCwi7i6YNx+4Pvf/54FHOj4ory/6IFyDMLNB6DlguqTDJQ0HriEd\ni98naRrwMPC5iHitO4X2+BAt6VzgOuBlSS+SmpK+DnwX+JmkG4C1wNWlyij3RLmOw1wLaxAjR0JL\nS7pW0/Dh5a3HzCzLIqJV0s3AQtIX//siYrmkG9PdMRf4G+Ag4AeSBDRHxOzOyu1xQETEvwE1Je6+\noDtl9GUNQmqvRRx8cHnrMTPLuoj4FTCjw7z/XvD/l4AvHUiZVX8mdak+CPBQVzOzclR9QJSqQYBP\nljMzK0dVB8TYsanW0NqabrsGYWbWe6o6IIYM2X84a8cahIe6mpn1XFUHBOzfD9GxBlFsqGtbW/nr\nNDMbDAZEQOT7IYrVIAoDYts2mDq1vUnKzMxKq/qAKDwXolgNorCJadUqePNNWLeu/PWamQ10Vf2L\ncrB/E1NXNYj8dZuWLy9/vWZmA13V1yAKm5i6qkG8+mr6u2JF+es1Mxvoqj4gCpuYulODmD3bNQgz\ns+6o+oA4kBrE6tVwxRWuQZiZdceACIht29ovzFdb235fsRrEJz/pGoSZWXcMiIDYurW99iC131d4\notyWLdDcDCeemIa5vvVW+es2MxvIqj4g8n0QHfsfYP8T5V59FY45JgXIzJluZjIz60rVB0THGkSh\nwiam1ath+vT0/3HHuZnJzKwrA+Y8iFI1iHwT0+rVqQYBrkGYmXXHoKlB5JuYIAWEaxBmZp2r+oDo\nrA+itjZ1TDc3f7CJyTUIM7POVTQgakr9YOkBqKtrb2LqWIOQ2msRhU1MRx4JmzbBe++Vv34zs4Gq\nogFROCS1p4YNg5Ej0wG/Y0BA6od4/fU0tPVDH0rzampSbWLlyvLXb2Y2UFU0IHpLXR1s2PDBJiZI\n8154oX2Ia547qs3MOjcgAmL8eFi/vngNIh8Q+f6HPHdUm5l1bsAERKkaxLhx7TWIQu6oNjPr3IAK\niFI1iMWLPxgQrkGYmXVuQAREXV3qpC5Vg9iz54MBceyx6dwI//yomVlxAyIgxo+HtrbSNQj4YB/E\nqFEweTL88Y99v31mZtVowAQElK5B1NXBIYd88L4TT4Rf/rJvt83MrFoNqIAoVYPoOMQ178474dvf\nTn0UnWlthYjyt9PMrJoMiICoq0t/i9UgJkyAGTOKP+744+Guu+DTn05nY3cUAT/9KUybBnfc0Xvb\na2ZWDQZEQHRWg/izP4O77y792Ouug/PPhy9+cf9awvLlcOGF8Hd/B/feCz/6Efz617273WZmWdYL\nl8urvM76IGpr9/8Z0mLuugvOPTf9HOmOHekSHPv2pVrDTTeliwrW1cG116ZzKj784d5/DmZmWaOo\nUOO6pOitdT/9NJx9drpqa0+vELtxIzz2GBx9dGqSmjIFhnSoXzU0wO9+l2oSvXGhQTOzAyWJiOiF\nK9l1Y10DISBWrIBTT4Xdu3uluJJaW1Oz09SpMGcOnHVW6uMwM+sv/RkQA6IPYuJEOPzwvl9PTU17\np/U//mP6e/LJ8PDDHuVkZgPPgKhBVEpLCzz5JHz1qzBpEnz/+2lkVKFdu2DRInjqKfjIR+CSS2DE\niMpsr5lVvwHRxCTpYuAuUi3lvoj4bof7qz4g8lpa4Ic/TOdUHH54anaaMCH9Hva//zuccQZ89KPQ\n2AivvJKG1d5wA8ye3Xm5bW3p/I3e+N2MrHnjDXj00TSNHAkf/3iaZswYmM93MNu9u+uBIlZaRJry\nfaKlAqKrY25ume8DlwC7gOsj4qXO1t0nASFpCLAKOB94A3gOuCYiVhQsM2ACIm/r1vTLde++C1u2\nwPDhaQht/jwNgHXr4IEHYO7cdHb3zTfDuHGNHHxwPS+9lE7aW7MmLbdhQ/qRoz/90xQqZ52V5i9f\nntYzYQIcdVSaDjooBUo+VGprD6wjPSKdC7JuXZpaW9PB+uij048y5e3bl65tlf8p1zFjig8vLqap\nCR58MD3/1atTbeqKK1I5v/kNPPEEvPNOI4cdVs/EiWm02AknwJ/8SZomTty/vH370qiyp59OgfPO\nO2kaMwZmzUrTlCnpciqrVqW/hx4Kp5ySmgZHjYLXXkvX5Fq3Lr1u27alYK+tTfv0oINg9Oj04Rwy\nJO2n995Lv4G+a1cqf9asVHOcPDnN27kzlbV8OSxdCsuWpXJOPTVNY8bA88/Dc8+l/rMZM+DMM9MX\niQkTUvm7d8NTTzVy1FH17/9iYuEXhnyISun9dcYZcMQR6XZbW9ovv/51etyHP5y2bezYdM2yN96A\nzZvTl5mTT4aTTkohnX/t165N++qPf0yX0c8/xxNOSDXlffvSaxaR1n3QQWm7a2ra34OrVqVBH48/\nnkYF1tWl53nMMWnf7tuXplGj0mVwpk9P219bmwaaDB2ayqupSfv9+ecbueyy+v2+POzalZ7Lzp3t\n78dt29J7a/Xq9FxGj4aDD06ftdNPT32IhTX4d9+F3/42/T92bJrWr4f/+I80rV2bPl8zZqTrt02Y\nkF6/0aNTmdOmpX1bU5PeN6+9lvZbbW3aVxMnpv2xfn2a3n47jbo85JC0XUOHtjdPDx2ayh09On3p\nbGxMr+GiRTBvHlxwQf41/2BAdPOYewlwc0RcJulM4O6IOKuzz2xfBcRZwB0RcUnu9m1AFCbaQAyI\nA9Hami7zcc898JvfNHDaaQ2cemo6eB19dPrwHnZYerM99FCali1LH9aZM9MHbdu2FCavvZbCKf9h\namtLB5gRI9oPbhFpfuGbcMSIdADZvj2VVVPTvt4hQ9IHe8OG9AHYvTst09qaDibDhqVp58704Tn3\n3HQQ37UrBcHmzekDW1OT1rlmDTzzTAqEa69NtYXC4IG0jbfe2sD11zfQ1JQ+/IsXwx/+kA54EelD\nN3lyWv4Pf0gHlnPOSR/Ugw9O0/bt6cC8dCm8+Wb6gB9zTPqp2Q0b4KWX4MUXYe/etK+nT2+v+Y0f\nnw4Se/akkN+yJT2n/P6D9v1XW5tGv+XX9dZb6bFjxqRLvBx3XDqwzpyZynnxxfQ8duxIB/QzzkjL\nrFyZ9s1zz6V15Ydmb9nSwIknNlBXl8rNv475Kb/P3nknPb6lJV0+ZvHi9MXiwgvTAerNN1Mw7NiR\n9t2UKekAtXZtWnbJknSwnjatfTryyDQddlh6HV55BV5+Oa1r+PD02knpPbFlSzrQtrWlbZTS4y69\nNE2nn572zcqV6cC9b18qY/jwtE35kH799XRfS0t677S1pfdbWxu8+24DbW0NTJiQ9u1bb6Vlp0xJ\nt4cOTds0blx6PY85Jr2m772Xtnnz5hQEr7wCl1+eXpMFC9Jrcs456bOwfXvanokT06jIc85J7501\na1LgrVqVPmf5LwFvvZUO+lu2pPfD3r3tQbdnT1pnU1PaJ9OmpX1yyCGpjHfeSWHR0tIe+M3Nqexd\nu9Jz/shH0mv4iU+k92n7l4KiAdGdY+6/AE9GxE9zt5cD9RHRVOo41VfnQRwKrC+4vQHookFlcKmp\nSW/Uyy9Pw2cbGoovd/zx8M1vpmnPnnRw7o6IdFDPv9nyH9yWlvY34d696cCTv17VqFEfbN7Zuzcd\nBEeNSsuMHLn/Mnv3poPe73+fvu2MG5cO4iedlD50LS3pQ37eeakzf/To0tsspfUcf3x7X85117U/\nn61b04GuqSl9mGbP3r92Vq0uvDDVJDvq7H3RUUQKvyVLUkhMm9b99efDprOmvc9+tvvlFTN5cpo+\n9rGePb6hAb7xjXQw3r49HWjHjz/w5sg33kjvw1Wr4K/+KtXwu2r+OuKI9IWmlL1727epgs2j3Tnm\ndlxmY25evweE9YHuhgO0H2xHjSpvnSNGpG9Rnd1/9tlp6ktSe9/OzJl9u65qlP/WfthhPXtsNRg2\nLH35mDSp52VMmQJf/nLvbROkz0D+9+4Hmr5sYmqIiItzt4s2MfX6is3MBoESTUxdHXM7NjGtAD5W\niSam54Dpkg4H3gSuAeYULtBfw7TMzAaBLo+5wHzgJuCnuUDZ2lk4QB8FRES0SroZWEj7kCv/wKeZ\nWR8odcyVdGO6O+ZGxOOSLpX0KmmY6xe6KrdiJ8qZmVm2VeRSG5IulrRC0ipJt1ZiGypF0lRJT0ha\nKullSV/JzZ8gaaGklZIWSBoA43O6JmmIpBckzc/dHpT7AUBSnaSfS1qee3+cORj3h6S/kPSKpCWS\nfiJp+GDaD5Luk9QkaUnBvJLPX9Ltklbn3jef6M1t6feAyJ3QcQ9wETALmCPpuP7ejgpqAb4aEbOA\ns4Gbcs//NmBRRMwAngBur+A29qdbgGUFtwfrfgC4G3g8ImYCJwMrGGT7Q9IU4MvAaRFxEqkZfA6D\naz/cTzo+Fir6/CUdD1wNzCSdIf0DqffGpVWiBjEbWB0RayOiGXgQuLIC21EREbEpf3p7ROwElgNT\nSftgXm6xecBVldnC/iNpKnAp8K8FswfdfgCQNA74aETcDxARLRGxjcG5P2qA0ZKGArWk8fqDZj9E\nxO+BdzvMLvX8rwAezL1fXgdW04vnnFUiIIqd0HFoBbaj4iQdAZwCPA1Myo8oiIhNwMTSjxww/hn4\na6CwI2ww7geAI4G3Jd2fa3KbK2kUg2x/RMQbwPeAdaRg2BYRixhk+6GIiSWef6mT33rFgLjcdzWS\nNAZ4CLglV5PoOFpgQI8ekHQZ0JSrTXVWJR7Q+6HAUOA04N6IOI00yuQ2Bt/7Yjzp2/LhwBRSTeI6\nBtl+6IZ+ef6VCIiNQOGFAKbm5g0auarzQ8CPI+KR3OwmSZNy908GNldq+/rJucAVktYA/xv4uKQf\nA5sG2X7I2wCsj4jnc7cfJgXGYHtfXACsiYgtEdEK/F/gHAbffuio1PPfCBSeP9+rx9NKBMT7J3RI\nGk46oWN+Bbajkn4ELIuIuwvmzQeuz/3/eeCRjg8aSCLi6xExLSKOIr0HnoiIzwGPMoj2Q16u+WC9\npGNzs84HljLI3hekpqWzJI3MdbaeTxrEMNj2g9i/Zl3q+c8HrsmN9DoSmA4822sbUYnzIHLXLb+b\n9hM67uz3jagQSecCvwNeJlUTA/g66UX9GenbwFrg6ojYWqnt7E+SPgb8ZURcIekgBu9+OJnUYT8M\nWEM6kam6oTZGAAAAbklEQVSGQbY/JN1B+tLQDLwI/DkwlkGyHyQ9ANQDB5MupHcH8P+An1Pk+Uu6\nHfgiaX/dEhELe21bfKKcmZkV405qMzMrygFhZmZFOSDMzKwoB4SZmRXlgDAzs6IcEGZmVpQDwszM\ninJAmJlZUf8feJyTwR4sulQAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1110423c8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(1)\n",
    "ax.plot(mean_losses)\n",
    "ax2 = plt.twinx()\n",
    "ax2.plot(np.arange(0, n_iterations+1, 10), mean_evals, 'ro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
